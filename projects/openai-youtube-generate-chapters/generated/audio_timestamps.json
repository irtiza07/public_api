{
  "duration": 589.0800170898438,
  "language": "english",
  "text": "Hey guys, welcome back to the channel. Today I'll show you how you can build a pipeline with OpenAI's Whisper and GPT-40 models to create a pipeline that takes in a YouTube video file and gives you a text file with YouTube chapters. You can see the example over here. We'll go through the diagram here and also the code that you see over here. I'll also link the code in the description below if you want to try it out for yourself. Okay, so what I'm going to do as we get started with this, I am going to run the program in the background. So we're going to quickly run the file and while the file is being generated, I'm going to go ahead and walk you through both the diagram and the code. So let's quickly kick off the file. So we're just going to run it here, give it the file path. And while this is running, we're going to go ahead and walk you through the diagram. All right, so this is the flow that the code goes through. We start with the YouTube video file, which is going to be an MP4 file. We turn that into an audio file, MP3 or any other format. We pass the audio file to OpenAI API, specifically the Whisper model, which can take in an audio file. It's going to return back the transcript with timestamps in a JSON format. Now, at this point, you should have the full transcript of your video, including the timestamp for each of the segment. Once you have that, we're going to soon look at a file, how that looks. But once you have that file, we feed it back to OpenAI, but this time we used a GPT 4.0 model. To the model, we're going to pass the JSON file that has the transcript and timestamps, but we're also going to add a prompt to tell it exactly how to generate the YouTube timestamp or YouTube chapters. So the response to this prompt should be a text file that we can save in our local directory as a text file, and you can copy paste it wherever you want to. So that's going to be the full flow. Now, let's take a look at the code to see how it's going to work exactly. Now, let's go step by step. So we're going to start with the video file, and so going from video to audio. So in the code, you're going to see that when you run the program for the first time, it automatically asks you for a directory or path to the local audio file. So in this example, I did not download anything from YouTube. Rather, this was a video that I recorded on my laptop, and I was about to upload to YouTube. This is more of a real world case because one of the most common thing most people do when uploading videos to YouTube is generate the timestamps and add them as YouTube chapters. So by going through this pipeline, most likely starting this video, I don't have to manually watch the whole video and write the timestamps myself. Instead, when the file is recorded in an MP4 file, I can quickly convert it into an MP3 file, feed it to the program, and everything should just work. So because of that, we don't have the conversion here, but you can use any tool. On a MacBook, you have QuickTime Pro, which you can use to export your MP4 file as audio-only MP3 file. So let's assume we have our MP3 file because most of the logic is going to be over here. Going back to the code, this is going to ask for a file path, which is going to be the path to your audio file. After that, it's going to call the transcriptions API with the WhisperOne model to get the transcription with timestamps, and you have to give it a response format of verbose JSON. Otherwise, you don't get the timestamp, you only get the transcript. Now, after this line, you should have the JSON file, and we're going to quickly take a look at the JSON file, which is going to be here in the generated directory. If I look at the audio timestamp, you're going to see this is the response that the Whisper model gives me, where for every timestamp, it gives me the text, right? So it uses some arbitrary logic to divide the file into different segments. For each segment, it's going to give me the start timestamp and how long the segment is for, and the text over here, okay? So if you go through it, you can see the text has been divided into multiple segments, and if I go all the way to the end, you're going to see that I have the end of the video where I say, take care, bye-bye, okay? So this is how the file looks like when you get it from OpenAI at this step. Now, once we have the transcript, you're going to see that this is the exact text or the exact words I say in the video. So even though we have the timestamp, we don't want to just upload this into YouTube as chapters. Instead, we want to pass the transcript back to the model and tell the model to generate a summary with timestamps. Now, let's see how I want to do that here. So you can see that once we have the transcribed audio or the transcription with timestamp, we call a function over here, generate chapters from transcript. So let's go ahead and take a look at that. That is the function here. Again, this is going to be linked in the description below if you want to take a look at it yourself. But all this one does is it opens the JSON file that I just showed you, and then it does some formatting to make sure both the starting and the starting timestamp is in minutes and seconds, or in other words, in, let me show you, in a format like this, okay? And the text, it parses out the text from, it parses out the text from here, okay? And then it appends it to a list. So at the end of the for loop, you end up with an array of summary lines where each line is the timestamp in minute colon second, and then the full text of what I said in the video, okay? Now we're going to pass this to the model. You're going to see, I pass it to the model over here, okay? Now I do prefix the summary with this prompt, which just says given the following transcript segment with timestamp, generate YouTube chapters in the format, and I give it the format here, and I tell it to only output the chapter list, nothing else. And then the summary goes here, okay? Once we have the prompt, we pass it to our 4.0 model and give it a system prompt as well. The model takes a while, but then it spits out the response, which we save in a chapters.txt file. And you're going to see chapters.txt file here. There you go. And I just finished running the program, and you're going to see when I initially run it, this is how it generates that transcript. So you can see the timestamp and then what I said in the video. And then at the end, I end up with the chapters that is saved in the file here. And once you look at the file, this is the transcript. So if you take a look at my last three or four videos, you're going to see YouTube chapters for each of them. And for each of them, the chapters were generated using the program that you see over here. Now let's do a quick recap by going back to the system diagram, which is going to be here. Going from left to right, we have our video file. We convert it into audio. We feed the audio to the Whisper model. The Whisper model generates a JSON file with the transcript and timestamp. We take that file and pass it to a different model, GPT-40, with a prompt that tells it to use the full transcript to generate YouTube summary in this format. It takes a while, but then the model returns a text object with a text data type response with the text looks something like this. We take it and then we save it in a text file called audio.chapters. And yeah, that's pretty much all. I am going to link the code in the description below. So if you have any questions, just let me know. Otherwise I will catch you folks in the next one. Take care, bye-bye.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.19958072900772095,
      "compression_ratio": 1.6065573692321777,
      "end": 2.0799999237060547,
      "no_speech_prob": 0.018507350236177444,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " Hey guys, welcome back to the channel.",
      "tokens": [
        50364,
        1911,
        1074,
        11,
        2928,
        646,
        281,
        264,
        2269,
        13,
        50468
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.19958072900772095,
      "compression_ratio": 1.6065573692321777,
      "end": 4.760000228881836,
      "no_speech_prob": 0.018507350236177444,
      "seek": 0,
      "start": 2.0799999237060547,
      "temperature": 0.0,
      "text": " Today I'll show you how you can build a pipeline",
      "tokens": [
        50468,
        2692,
        286,
        603,
        855,
        291,
        577,
        291,
        393,
        1322,
        257,
        15517,
        50602
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.19958072900772095,
      "compression_ratio": 1.6065573692321777,
      "end": 9.319999694824219,
      "no_speech_prob": 0.018507350236177444,
      "seek": 0,
      "start": 4.760000228881836,
      "temperature": 0.0,
      "text": " with OpenAI's Whisper and GPT-40 models",
      "tokens": [
        50602,
        365,
        7238,
        48698,
        311,
        41132,
        610,
        293,
        26039,
        51,
        12,
        5254,
        5245,
        50830
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.19958072900772095,
      "compression_ratio": 1.6065573692321777,
      "end": 14.079999923706055,
      "no_speech_prob": 0.018507350236177444,
      "seek": 0,
      "start": 9.319999694824219,
      "temperature": 0.0,
      "text": " to create a pipeline that takes in a YouTube video file",
      "tokens": [
        50830,
        281,
        1884,
        257,
        15517,
        300,
        2516,
        294,
        257,
        3088,
        960,
        3991,
        51068
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.19958072900772095,
      "compression_ratio": 1.6065573692321777,
      "end": 18.34000015258789,
      "no_speech_prob": 0.018507350236177444,
      "seek": 0,
      "start": 14.079999923706055,
      "temperature": 0.0,
      "text": " and gives you a text file with YouTube chapters.",
      "tokens": [
        51068,
        293,
        2709,
        291,
        257,
        2487,
        3991,
        365,
        3088,
        20013,
        13,
        51281
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.19958072900772095,
      "compression_ratio": 1.6065573692321777,
      "end": 20.040000915527344,
      "no_speech_prob": 0.018507350236177444,
      "seek": 0,
      "start": 18.34000015258789,
      "temperature": 0.0,
      "text": " You can see the example over here.",
      "tokens": [
        51281,
        509,
        393,
        536,
        264,
        1365,
        670,
        510,
        13,
        51366
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.19958072900772095,
      "compression_ratio": 1.6065573692321777,
      "end": 22.719999313354492,
      "no_speech_prob": 0.018507350236177444,
      "seek": 0,
      "start": 21.079999923706055,
      "temperature": 0.0,
      "text": " We'll go through the diagram here",
      "tokens": [
        51418,
        492,
        603,
        352,
        807,
        264,
        10686,
        510,
        51500
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.19958072900772095,
      "compression_ratio": 1.6065573692321777,
      "end": 26.579999923706055,
      "no_speech_prob": 0.018507350236177444,
      "seek": 0,
      "start": 22.719999313354492,
      "temperature": 0.0,
      "text": " and also the code that you see over here.",
      "tokens": [
        51500,
        293,
        611,
        264,
        3089,
        300,
        291,
        536,
        670,
        510,
        13,
        51693
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.19958072900772095,
      "compression_ratio": 1.6065573692321777,
      "end": 28.68000030517578,
      "no_speech_prob": 0.018507350236177444,
      "seek": 0,
      "start": 26.579999923706055,
      "temperature": 0.0,
      "text": " I'll also link the code in the description below",
      "tokens": [
        51693,
        286,
        603,
        611,
        2113,
        264,
        3089,
        294,
        264,
        3855,
        2507,
        51798
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.2157866358757019,
      "compression_ratio": 1.7990868091583252,
      "end": 31.239999771118164,
      "no_speech_prob": 7.843509956728667e-05,
      "seek": 2868,
      "start": 28.68000030517578,
      "temperature": 0.0,
      "text": " if you want to try it out for yourself.",
      "tokens": [
        50364,
        498,
        291,
        528,
        281,
        853,
        309,
        484,
        337,
        1803,
        13,
        50492
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.2157866358757019,
      "compression_ratio": 1.7990868091583252,
      "end": 34.880001068115234,
      "no_speech_prob": 7.843509956728667e-05,
      "seek": 2868,
      "start": 31.239999771118164,
      "temperature": 0.0,
      "text": " Okay, so what I'm going to do as we get started with this,",
      "tokens": [
        50492,
        1033,
        11,
        370,
        437,
        286,
        478,
        516,
        281,
        360,
        382,
        321,
        483,
        1409,
        365,
        341,
        11,
        50674
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.2157866358757019,
      "compression_ratio": 1.7990868091583252,
      "end": 37.79999923706055,
      "no_speech_prob": 7.843509956728667e-05,
      "seek": 2868,
      "start": 34.880001068115234,
      "temperature": 0.0,
      "text": " I am going to run the program in the background.",
      "tokens": [
        50674,
        286,
        669,
        516,
        281,
        1190,
        264,
        1461,
        294,
        264,
        3678,
        13,
        50820
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.2157866358757019,
      "compression_ratio": 1.7990868091583252,
      "end": 39.63999938964844,
      "no_speech_prob": 7.843509956728667e-05,
      "seek": 2868,
      "start": 37.79999923706055,
      "temperature": 0.0,
      "text": " So we're going to quickly run the file",
      "tokens": [
        50820,
        407,
        321,
        434,
        516,
        281,
        2661,
        1190,
        264,
        3991,
        50912
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.2157866358757019,
      "compression_ratio": 1.7990868091583252,
      "end": 42.5,
      "no_speech_prob": 7.843509956728667e-05,
      "seek": 2868,
      "start": 39.63999938964844,
      "temperature": 0.0,
      "text": " and while the file is being generated,",
      "tokens": [
        50912,
        293,
        1339,
        264,
        3991,
        307,
        885,
        10833,
        11,
        51055
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.2157866358757019,
      "compression_ratio": 1.7990868091583252,
      "end": 44.70000076293945,
      "no_speech_prob": 7.843509956728667e-05,
      "seek": 2868,
      "start": 42.5,
      "temperature": 0.0,
      "text": " I'm going to go ahead and walk you through",
      "tokens": [
        51055,
        286,
        478,
        516,
        281,
        352,
        2286,
        293,
        1792,
        291,
        807,
        51165
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.2157866358757019,
      "compression_ratio": 1.7990868091583252,
      "end": 47.15999984741211,
      "no_speech_prob": 7.843509956728667e-05,
      "seek": 2868,
      "start": 44.70000076293945,
      "temperature": 0.0,
      "text": " both the diagram and the code.",
      "tokens": [
        51165,
        1293,
        264,
        10686,
        293,
        264,
        3089,
        13,
        51288
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.2157866358757019,
      "compression_ratio": 1.7990868091583252,
      "end": 51.599998474121094,
      "no_speech_prob": 7.843509956728667e-05,
      "seek": 2868,
      "start": 47.15999984741211,
      "temperature": 0.0,
      "text": " So let's quickly kick off the file.",
      "tokens": [
        51288,
        407,
        718,
        311,
        2661,
        4437,
        766,
        264,
        3991,
        13,
        51510
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.2157866358757019,
      "compression_ratio": 1.7990868091583252,
      "end": 55.119998931884766,
      "no_speech_prob": 7.843509956728667e-05,
      "seek": 2868,
      "start": 51.599998474121094,
      "temperature": 0.0,
      "text": " So we're just going to run it here, give it the file path.",
      "tokens": [
        51510,
        407,
        321,
        434,
        445,
        516,
        281,
        1190,
        309,
        510,
        11,
        976,
        309,
        264,
        3991,
        3100,
        13,
        51686
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.22313477098941803,
      "compression_ratio": 1.573394536972046,
      "end": 59.36000061035156,
      "no_speech_prob": 6.013939855620265e-05,
      "seek": 5512,
      "start": 55.52000045776367,
      "temperature": 0.0,
      "text": " And while this is running,",
      "tokens": [
        50384,
        400,
        1339,
        341,
        307,
        2614,
        11,
        50576
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.22313477098941803,
      "compression_ratio": 1.573394536972046,
      "end": 62.58000183105469,
      "no_speech_prob": 6.013939855620265e-05,
      "seek": 5512,
      "start": 59.36000061035156,
      "temperature": 0.0,
      "text": " we're going to go ahead and walk you through the diagram.",
      "tokens": [
        50576,
        321,
        434,
        516,
        281,
        352,
        2286,
        293,
        1792,
        291,
        807,
        264,
        10686,
        13,
        50737
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.22313477098941803,
      "compression_ratio": 1.573394536972046,
      "end": 67.95999908447266,
      "no_speech_prob": 6.013939855620265e-05,
      "seek": 5512,
      "start": 63.47999954223633,
      "temperature": 0.0,
      "text": " All right, so this is the flow that the code goes through.",
      "tokens": [
        50782,
        1057,
        558,
        11,
        370,
        341,
        307,
        264,
        3095,
        300,
        264,
        3089,
        1709,
        807,
        13,
        51006
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.22313477098941803,
      "compression_ratio": 1.573394536972046,
      "end": 69.72000122070312,
      "no_speech_prob": 6.013939855620265e-05,
      "seek": 5512,
      "start": 67.95999908447266,
      "temperature": 0.0,
      "text": " We start with the YouTube video file,",
      "tokens": [
        51006,
        492,
        722,
        365,
        264,
        3088,
        960,
        3991,
        11,
        51094
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.22313477098941803,
      "compression_ratio": 1.573394536972046,
      "end": 72.0,
      "no_speech_prob": 6.013939855620265e-05,
      "seek": 5512,
      "start": 69.72000122070312,
      "temperature": 0.0,
      "text": " which is going to be an MP4 file.",
      "tokens": [
        51094,
        597,
        307,
        516,
        281,
        312,
        364,
        14146,
        19,
        3991,
        13,
        51208
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.22313477098941803,
      "compression_ratio": 1.573394536972046,
      "end": 77.0,
      "no_speech_prob": 6.013939855620265e-05,
      "seek": 5512,
      "start": 72.0,
      "temperature": 0.0,
      "text": " We turn that into an audio file, MP3 or any other format.",
      "tokens": [
        51208,
        492,
        1261,
        300,
        666,
        364,
        6278,
        3991,
        11,
        14146,
        18,
        420,
        604,
        661,
        7877,
        13,
        51458
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.22313477098941803,
      "compression_ratio": 1.573394536972046,
      "end": 81.4000015258789,
      "no_speech_prob": 6.013939855620265e-05,
      "seek": 5512,
      "start": 77.0199966430664,
      "temperature": 0.0,
      "text": " We pass the audio file to OpenAI API,",
      "tokens": [
        51459,
        492,
        1320,
        264,
        6278,
        3991,
        281,
        7238,
        48698,
        9362,
        11,
        51678
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.22313477098941803,
      "compression_ratio": 1.573394536972046,
      "end": 83.5999984741211,
      "no_speech_prob": 6.013939855620265e-05,
      "seek": 5512,
      "start": 81.4000015258789,
      "temperature": 0.0,
      "text": " specifically the Whisper model,",
      "tokens": [
        51678,
        4682,
        264,
        41132,
        610,
        2316,
        11,
        51788
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.20582932233810425,
      "compression_ratio": 1.6284403800964355,
      "end": 86.33999633789062,
      "no_speech_prob": 0.00010229956387775019,
      "seek": 8360,
      "start": 83.5999984741211,
      "temperature": 0.0,
      "text": " which can take in an audio file.",
      "tokens": [
        50364,
        597,
        393,
        747,
        294,
        364,
        6278,
        3991,
        13,
        50501
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.20582932233810425,
      "compression_ratio": 1.6284403800964355,
      "end": 91.0,
      "no_speech_prob": 0.00010229956387775019,
      "seek": 8360,
      "start": 86.33999633789062,
      "temperature": 0.0,
      "text": " It's going to return back the transcript with timestamps",
      "tokens": [
        50501,
        467,
        311,
        516,
        281,
        2736,
        646,
        264,
        24444,
        365,
        49108,
        23150,
        50734
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.20582932233810425,
      "compression_ratio": 1.6284403800964355,
      "end": 93.08000183105469,
      "no_speech_prob": 0.00010229956387775019,
      "seek": 8360,
      "start": 91.0,
      "temperature": 0.0,
      "text": " in a JSON format.",
      "tokens": [
        50734,
        294,
        257,
        31828,
        7877,
        13,
        50838
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.20582932233810425,
      "compression_ratio": 1.6284403800964355,
      "end": 94.87999725341797,
      "no_speech_prob": 0.00010229956387775019,
      "seek": 8360,
      "start": 94.04000091552734,
      "temperature": 0.0,
      "text": " Now, at this point,",
      "tokens": [
        50886,
        823,
        11,
        412,
        341,
        935,
        11,
        50928
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.20582932233810425,
      "compression_ratio": 1.6284403800964355,
      "end": 98.5199966430664,
      "no_speech_prob": 0.00010229956387775019,
      "seek": 8360,
      "start": 94.87999725341797,
      "temperature": 0.0,
      "text": " you should have the full transcript of your video,",
      "tokens": [
        50928,
        291,
        820,
        362,
        264,
        1577,
        24444,
        295,
        428,
        960,
        11,
        51110
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.20582932233810425,
      "compression_ratio": 1.6284403800964355,
      "end": 103.08000183105469,
      "no_speech_prob": 0.00010229956387775019,
      "seek": 8360,
      "start": 98.5199966430664,
      "temperature": 0.0,
      "text": " including the timestamp for each of the segment.",
      "tokens": [
        51110,
        3009,
        264,
        49108,
        1215,
        337,
        1184,
        295,
        264,
        9469,
        13,
        51338
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.20582932233810425,
      "compression_ratio": 1.6284403800964355,
      "end": 104.91999816894531,
      "no_speech_prob": 0.00010229956387775019,
      "seek": 8360,
      "start": 104.0,
      "temperature": 0.0,
      "text": " Once you have that,",
      "tokens": [
        51384,
        3443,
        291,
        362,
        300,
        11,
        51430
      ]
    },
    {
      "id": 33,
      "avg_logprob": -0.20582932233810425,
      "compression_ratio": 1.6284403800964355,
      "end": 107.5199966430664,
      "no_speech_prob": 0.00010229956387775019,
      "seek": 8360,
      "start": 104.91999816894531,
      "temperature": 0.0,
      "text": " we're going to soon look at a file, how that looks.",
      "tokens": [
        51430,
        321,
        434,
        516,
        281,
        2321,
        574,
        412,
        257,
        3991,
        11,
        577,
        300,
        1542,
        13,
        51560
      ]
    },
    {
      "id": 34,
      "avg_logprob": -0.20582932233810425,
      "compression_ratio": 1.6284403800964355,
      "end": 112.5199966430664,
      "no_speech_prob": 0.00010229956387775019,
      "seek": 8360,
      "start": 107.5199966430664,
      "temperature": 0.0,
      "text": " But once you have that file, we feed it back to OpenAI,",
      "tokens": [
        51560,
        583,
        1564,
        291,
        362,
        300,
        3991,
        11,
        321,
        3154,
        309,
        646,
        281,
        7238,
        48698,
        11,
        51810
      ]
    },
    {
      "id": 35,
      "avg_logprob": -0.1880640983581543,
      "compression_ratio": 1.6203703880310059,
      "end": 116.95999908447266,
      "no_speech_prob": 8.750279812375084e-05,
      "seek": 11252,
      "start": 113.08000183105469,
      "temperature": 0.0,
      "text": " but this time we used a GPT 4.0 model.",
      "tokens": [
        50392,
        457,
        341,
        565,
        321,
        1143,
        257,
        26039,
        51,
        1017,
        13,
        15,
        2316,
        13,
        50586
      ]
    },
    {
      "id": 36,
      "avg_logprob": -0.1880640983581543,
      "compression_ratio": 1.6203703880310059,
      "end": 119.68000030517578,
      "no_speech_prob": 8.750279812375084e-05,
      "seek": 11252,
      "start": 116.95999908447266,
      "temperature": 0.0,
      "text": " To the model, we're going to pass the JSON file",
      "tokens": [
        50586,
        1407,
        264,
        2316,
        11,
        321,
        434,
        516,
        281,
        1320,
        264,
        31828,
        3991,
        50722
      ]
    },
    {
      "id": 37,
      "avg_logprob": -0.1880640983581543,
      "compression_ratio": 1.6203703880310059,
      "end": 122.95999908447266,
      "no_speech_prob": 8.750279812375084e-05,
      "seek": 11252,
      "start": 119.68000030517578,
      "temperature": 0.0,
      "text": " that has the transcript and timestamps,",
      "tokens": [
        50722,
        300,
        575,
        264,
        24444,
        293,
        49108,
        23150,
        11,
        50886
      ]
    },
    {
      "id": 38,
      "avg_logprob": -0.1880640983581543,
      "compression_ratio": 1.6203703880310059,
      "end": 126.44000244140625,
      "no_speech_prob": 8.750279812375084e-05,
      "seek": 11252,
      "start": 122.95999908447266,
      "temperature": 0.0,
      "text": " but we're also going to add a prompt to tell it exactly",
      "tokens": [
        50886,
        457,
        321,
        434,
        611,
        516,
        281,
        909,
        257,
        12391,
        281,
        980,
        309,
        2293,
        51060
      ]
    },
    {
      "id": 39,
      "avg_logprob": -0.1880640983581543,
      "compression_ratio": 1.6203703880310059,
      "end": 131.44000244140625,
      "no_speech_prob": 8.750279812375084e-05,
      "seek": 11252,
      "start": 126.44000244140625,
      "temperature": 0.0,
      "text": " how to generate the YouTube timestamp or YouTube chapters.",
      "tokens": [
        51060,
        577,
        281,
        8460,
        264,
        3088,
        49108,
        1215,
        420,
        3088,
        20013,
        13,
        51310
      ]
    },
    {
      "id": 40,
      "avg_logprob": -0.1880640983581543,
      "compression_ratio": 1.6203703880310059,
      "end": 135.67999267578125,
      "no_speech_prob": 8.750279812375084e-05,
      "seek": 11252,
      "start": 131.72000122070312,
      "temperature": 0.0,
      "text": " So the response to this prompt should be a text file",
      "tokens": [
        51324,
        407,
        264,
        4134,
        281,
        341,
        12391,
        820,
        312,
        257,
        2487,
        3991,
        51522
      ]
    },
    {
      "id": 41,
      "avg_logprob": -0.1880640983581543,
      "compression_ratio": 1.6203703880310059,
      "end": 140.67999267578125,
      "no_speech_prob": 8.750279812375084e-05,
      "seek": 11252,
      "start": 135.67999267578125,
      "temperature": 0.0,
      "text": " that we can save in our local directory as a text file,",
      "tokens": [
        51522,
        300,
        321,
        393,
        3155,
        294,
        527,
        2654,
        21120,
        382,
        257,
        2487,
        3991,
        11,
        51772
      ]
    },
    {
      "id": 42,
      "avg_logprob": -0.21910911798477173,
      "compression_ratio": 1.7376238107681274,
      "end": 143.9600067138672,
      "no_speech_prob": 9.972915904654656e-06,
      "seek": 14068,
      "start": 140.9199981689453,
      "temperature": 0.0,
      "text": " and you can copy paste it wherever you want to.",
      "tokens": [
        50376,
        293,
        291,
        393,
        5055,
        9163,
        309,
        8660,
        291,
        528,
        281,
        13,
        50528
      ]
    },
    {
      "id": 43,
      "avg_logprob": -0.21910911798477173,
      "compression_ratio": 1.7376238107681274,
      "end": 146.97999572753906,
      "no_speech_prob": 9.972915904654656e-06,
      "seek": 14068,
      "start": 145.0,
      "temperature": 0.0,
      "text": " So that's going to be the full flow.",
      "tokens": [
        50580,
        407,
        300,
        311,
        516,
        281,
        312,
        264,
        1577,
        3095,
        13,
        50679
      ]
    },
    {
      "id": 44,
      "avg_logprob": -0.21910911798477173,
      "compression_ratio": 1.7376238107681274,
      "end": 149.44000244140625,
      "no_speech_prob": 9.972915904654656e-06,
      "seek": 14068,
      "start": 146.97999572753906,
      "temperature": 0.0,
      "text": " Now, let's take a look at the code",
      "tokens": [
        50679,
        823,
        11,
        718,
        311,
        747,
        257,
        574,
        412,
        264,
        3089,
        50802
      ]
    },
    {
      "id": 45,
      "avg_logprob": -0.21910911798477173,
      "compression_ratio": 1.7376238107681274,
      "end": 151.97999572753906,
      "no_speech_prob": 9.972915904654656e-06,
      "seek": 14068,
      "start": 149.44000244140625,
      "temperature": 0.0,
      "text": " to see how it's going to work exactly.",
      "tokens": [
        50802,
        281,
        536,
        577,
        309,
        311,
        516,
        281,
        589,
        2293,
        13,
        50929
      ]
    },
    {
      "id": 46,
      "avg_logprob": -0.21910911798477173,
      "compression_ratio": 1.7376238107681274,
      "end": 154.83999633789062,
      "no_speech_prob": 9.972915904654656e-06,
      "seek": 14068,
      "start": 153.27999877929688,
      "temperature": 0.0,
      "text": " Now, let's go step by step.",
      "tokens": [
        50994,
        823,
        11,
        718,
        311,
        352,
        1823,
        538,
        1823,
        13,
        51072
      ]
    },
    {
      "id": 47,
      "avg_logprob": -0.21910911798477173,
      "compression_ratio": 1.7376238107681274,
      "end": 157.1199951171875,
      "no_speech_prob": 9.972915904654656e-06,
      "seek": 14068,
      "start": 154.83999633789062,
      "temperature": 0.0,
      "text": " So we're going to start with the video file,",
      "tokens": [
        51072,
        407,
        321,
        434,
        516,
        281,
        722,
        365,
        264,
        960,
        3991,
        11,
        51186
      ]
    },
    {
      "id": 48,
      "avg_logprob": -0.21910911798477173,
      "compression_ratio": 1.7376238107681274,
      "end": 160.32000732421875,
      "no_speech_prob": 9.972915904654656e-06,
      "seek": 14068,
      "start": 157.1199951171875,
      "temperature": 0.0,
      "text": " and so going from video to audio.",
      "tokens": [
        51186,
        293,
        370,
        516,
        490,
        960,
        281,
        6278,
        13,
        51346
      ]
    },
    {
      "id": 49,
      "avg_logprob": -0.21910911798477173,
      "compression_ratio": 1.7376238107681274,
      "end": 163.24000549316406,
      "no_speech_prob": 9.972915904654656e-06,
      "seek": 14068,
      "start": 160.32000732421875,
      "temperature": 0.0,
      "text": " So in the code, you're going to see that",
      "tokens": [
        51346,
        407,
        294,
        264,
        3089,
        11,
        291,
        434,
        516,
        281,
        536,
        300,
        51492
      ]
    },
    {
      "id": 50,
      "avg_logprob": -0.21910911798477173,
      "compression_ratio": 1.7376238107681274,
      "end": 166.16000366210938,
      "no_speech_prob": 9.972915904654656e-06,
      "seek": 14068,
      "start": 163.24000549316406,
      "temperature": 0.0,
      "text": " when you run the program for the first time,",
      "tokens": [
        51492,
        562,
        291,
        1190,
        264,
        1461,
        337,
        264,
        700,
        565,
        11,
        51638
      ]
    },
    {
      "id": 51,
      "avg_logprob": -0.20354798436164856,
      "compression_ratio": 1.5963302850723267,
      "end": 170.63999938964844,
      "no_speech_prob": 5.73873876419384e-05,
      "seek": 16616,
      "start": 166.1999969482422,
      "temperature": 0.0,
      "text": " it automatically asks you for a directory or path",
      "tokens": [
        50366,
        309,
        6772,
        8962,
        291,
        337,
        257,
        21120,
        420,
        3100,
        50588
      ]
    },
    {
      "id": 52,
      "avg_logprob": -0.20354798436164856,
      "compression_ratio": 1.5963302850723267,
      "end": 172.44000244140625,
      "no_speech_prob": 5.73873876419384e-05,
      "seek": 16616,
      "start": 170.63999938964844,
      "temperature": 0.0,
      "text": " to the local audio file.",
      "tokens": [
        50588,
        281,
        264,
        2654,
        6278,
        3991,
        13,
        50678
      ]
    },
    {
      "id": 53,
      "avg_logprob": -0.20354798436164856,
      "compression_ratio": 1.5963302850723267,
      "end": 173.60000610351562,
      "no_speech_prob": 5.73873876419384e-05,
      "seek": 16616,
      "start": 172.44000244140625,
      "temperature": 0.0,
      "text": " So in this example,",
      "tokens": [
        50678,
        407,
        294,
        341,
        1365,
        11,
        50736
      ]
    },
    {
      "id": 54,
      "avg_logprob": -0.20354798436164856,
      "compression_ratio": 1.5963302850723267,
      "end": 176.83999633789062,
      "no_speech_prob": 5.73873876419384e-05,
      "seek": 16616,
      "start": 173.60000610351562,
      "temperature": 0.0,
      "text": " I did not download anything from YouTube.",
      "tokens": [
        50736,
        286,
        630,
        406,
        5484,
        1340,
        490,
        3088,
        13,
        50898
      ]
    },
    {
      "id": 55,
      "avg_logprob": -0.20354798436164856,
      "compression_ratio": 1.5963302850723267,
      "end": 180.63999938964844,
      "no_speech_prob": 5.73873876419384e-05,
      "seek": 16616,
      "start": 176.83999633789062,
      "temperature": 0.0,
      "text": " Rather, this was a video that I recorded on my laptop,",
      "tokens": [
        50898,
        16571,
        11,
        341,
        390,
        257,
        960,
        300,
        286,
        8287,
        322,
        452,
        10732,
        11,
        51088
      ]
    },
    {
      "id": 56,
      "avg_logprob": -0.20354798436164856,
      "compression_ratio": 1.5963302850723267,
      "end": 183.52000427246094,
      "no_speech_prob": 5.73873876419384e-05,
      "seek": 16616,
      "start": 180.63999938964844,
      "temperature": 0.0,
      "text": " and I was about to upload to YouTube.",
      "tokens": [
        51088,
        293,
        286,
        390,
        466,
        281,
        6580,
        281,
        3088,
        13,
        51232
      ]
    },
    {
      "id": 57,
      "avg_logprob": -0.20354798436164856,
      "compression_ratio": 1.5963302850723267,
      "end": 186.63999938964844,
      "no_speech_prob": 5.73873876419384e-05,
      "seek": 16616,
      "start": 184.63999938964844,
      "temperature": 0.0,
      "text": " This is more of a real world case",
      "tokens": [
        51288,
        639,
        307,
        544,
        295,
        257,
        957,
        1002,
        1389,
        51388
      ]
    },
    {
      "id": 58,
      "avg_logprob": -0.20354798436164856,
      "compression_ratio": 1.5963302850723267,
      "end": 190.0800018310547,
      "no_speech_prob": 5.73873876419384e-05,
      "seek": 16616,
      "start": 186.63999938964844,
      "temperature": 0.0,
      "text": " because one of the most common thing most people do",
      "tokens": [
        51388,
        570,
        472,
        295,
        264,
        881,
        2689,
        551,
        881,
        561,
        360,
        51560
      ]
    },
    {
      "id": 59,
      "avg_logprob": -0.20354798436164856,
      "compression_ratio": 1.5963302850723267,
      "end": 192.32000732421875,
      "no_speech_prob": 5.73873876419384e-05,
      "seek": 16616,
      "start": 190.0800018310547,
      "temperature": 0.0,
      "text": " when uploading videos to YouTube",
      "tokens": [
        51560,
        562,
        27301,
        2145,
        281,
        3088,
        51672
      ]
    },
    {
      "id": 60,
      "avg_logprob": -0.18834051489830017,
      "compression_ratio": 1.540772557258606,
      "end": 196.60000610351562,
      "no_speech_prob": 0.00034062773920595646,
      "seek": 19232,
      "start": 192.32000732421875,
      "temperature": 0.0,
      "text": " is generate the timestamps",
      "tokens": [
        50364,
        307,
        8460,
        264,
        49108,
        23150,
        50578
      ]
    },
    {
      "id": 61,
      "avg_logprob": -0.18834051489830017,
      "compression_ratio": 1.540772557258606,
      "end": 199.0800018310547,
      "no_speech_prob": 0.00034062773920595646,
      "seek": 19232,
      "start": 196.60000610351562,
      "temperature": 0.0,
      "text": " and add them as YouTube chapters.",
      "tokens": [
        50578,
        293,
        909,
        552,
        382,
        3088,
        20013,
        13,
        50702
      ]
    },
    {
      "id": 62,
      "avg_logprob": -0.18834051489830017,
      "compression_ratio": 1.540772557258606,
      "end": 200.97999572753906,
      "no_speech_prob": 0.00034062773920595646,
      "seek": 19232,
      "start": 199.0800018310547,
      "temperature": 0.0,
      "text": " So by going through this pipeline,",
      "tokens": [
        50702,
        407,
        538,
        516,
        807,
        341,
        15517,
        11,
        50797
      ]
    },
    {
      "id": 63,
      "avg_logprob": -0.18834051489830017,
      "compression_ratio": 1.540772557258606,
      "end": 203.1199951171875,
      "no_speech_prob": 0.00034062773920595646,
      "seek": 19232,
      "start": 200.97999572753906,
      "temperature": 0.0,
      "text": " most likely starting this video,",
      "tokens": [
        50797,
        881,
        3700,
        2891,
        341,
        960,
        11,
        50904
      ]
    },
    {
      "id": 64,
      "avg_logprob": -0.18834051489830017,
      "compression_ratio": 1.540772557258606,
      "end": 207.36000061035156,
      "no_speech_prob": 0.00034062773920595646,
      "seek": 19232,
      "start": 203.1199951171875,
      "temperature": 0.0,
      "text": " I don't have to manually watch the whole video",
      "tokens": [
        50904,
        286,
        500,
        380,
        362,
        281,
        16945,
        1159,
        264,
        1379,
        960,
        51116
      ]
    },
    {
      "id": 65,
      "avg_logprob": -0.18834051489830017,
      "compression_ratio": 1.540772557258606,
      "end": 209.9199981689453,
      "no_speech_prob": 0.00034062773920595646,
      "seek": 19232,
      "start": 207.36000061035156,
      "temperature": 0.0,
      "text": " and write the timestamps myself.",
      "tokens": [
        51116,
        293,
        2464,
        264,
        49108,
        23150,
        2059,
        13,
        51244
      ]
    },
    {
      "id": 66,
      "avg_logprob": -0.18834051489830017,
      "compression_ratio": 1.540772557258606,
      "end": 213.97999572753906,
      "no_speech_prob": 0.00034062773920595646,
      "seek": 19232,
      "start": 209.9199981689453,
      "temperature": 0.0,
      "text": " Instead, when the file is recorded in an MP4 file,",
      "tokens": [
        51244,
        7156,
        11,
        562,
        264,
        3991,
        307,
        8287,
        294,
        364,
        14146,
        19,
        3991,
        11,
        51447
      ]
    },
    {
      "id": 67,
      "avg_logprob": -0.18834051489830017,
      "compression_ratio": 1.540772557258606,
      "end": 217.16000366210938,
      "no_speech_prob": 0.00034062773920595646,
      "seek": 19232,
      "start": 213.97999572753906,
      "temperature": 0.0,
      "text": " I can quickly convert it into an MP3 file,",
      "tokens": [
        51447,
        286,
        393,
        2661,
        7620,
        309,
        666,
        364,
        14146,
        18,
        3991,
        11,
        51606
      ]
    },
    {
      "id": 68,
      "avg_logprob": -0.18834051489830017,
      "compression_ratio": 1.540772557258606,
      "end": 220.17999267578125,
      "no_speech_prob": 0.00034062773920595646,
      "seek": 19232,
      "start": 217.16000366210938,
      "temperature": 0.0,
      "text": " feed it to the program, and everything should just work.",
      "tokens": [
        51606,
        3154,
        309,
        281,
        264,
        1461,
        11,
        293,
        1203,
        820,
        445,
        589,
        13,
        51757
      ]
    },
    {
      "id": 69,
      "avg_logprob": -0.16896255314350128,
      "compression_ratio": 1.7035398483276367,
      "end": 224.1199951171875,
      "no_speech_prob": 2.8409667720552534e-05,
      "seek": 22018,
      "start": 221.05999755859375,
      "temperature": 0.0,
      "text": " So because of that, we don't have the conversion here,",
      "tokens": [
        50408,
        407,
        570,
        295,
        300,
        11,
        321,
        500,
        380,
        362,
        264,
        14298,
        510,
        11,
        50561
      ]
    },
    {
      "id": 70,
      "avg_logprob": -0.16896255314350128,
      "compression_ratio": 1.7035398483276367,
      "end": 225.86000061035156,
      "no_speech_prob": 2.8409667720552534e-05,
      "seek": 22018,
      "start": 224.1199951171875,
      "temperature": 0.0,
      "text": " but you can use any tool.",
      "tokens": [
        50561,
        457,
        291,
        393,
        764,
        604,
        2290,
        13,
        50648
      ]
    },
    {
      "id": 71,
      "avg_logprob": -0.16896255314350128,
      "compression_ratio": 1.7035398483276367,
      "end": 228.17999267578125,
      "no_speech_prob": 2.8409667720552534e-05,
      "seek": 22018,
      "start": 225.86000061035156,
      "temperature": 0.0,
      "text": " On a MacBook, you have QuickTime Pro,",
      "tokens": [
        50648,
        1282,
        257,
        31737,
        11,
        291,
        362,
        12101,
        22233,
        1705,
        11,
        50764
      ]
    },
    {
      "id": 72,
      "avg_logprob": -0.16896255314350128,
      "compression_ratio": 1.7035398483276367,
      "end": 231.5,
      "no_speech_prob": 2.8409667720552534e-05,
      "seek": 22018,
      "start": 228.17999267578125,
      "temperature": 0.0,
      "text": " which you can use to export your MP4 file",
      "tokens": [
        50764,
        597,
        291,
        393,
        764,
        281,
        10725,
        428,
        14146,
        19,
        3991,
        50930
      ]
    },
    {
      "id": 73,
      "avg_logprob": -0.16896255314350128,
      "compression_ratio": 1.7035398483276367,
      "end": 234.25999450683594,
      "no_speech_prob": 2.8409667720552534e-05,
      "seek": 22018,
      "start": 231.5,
      "temperature": 0.0,
      "text": " as audio-only MP3 file.",
      "tokens": [
        50930,
        382,
        6278,
        12,
        25202,
        14146,
        18,
        3991,
        13,
        51068
      ]
    },
    {
      "id": 74,
      "avg_logprob": -0.16896255314350128,
      "compression_ratio": 1.7035398483276367,
      "end": 237.02000427246094,
      "no_speech_prob": 2.8409667720552534e-05,
      "seek": 22018,
      "start": 234.25999450683594,
      "temperature": 0.0,
      "text": " So let's assume we have our MP3 file",
      "tokens": [
        51068,
        407,
        718,
        311,
        6552,
        321,
        362,
        527,
        14146,
        18,
        3991,
        51206
      ]
    },
    {
      "id": 75,
      "avg_logprob": -0.16896255314350128,
      "compression_ratio": 1.7035398483276367,
      "end": 239.52000427246094,
      "no_speech_prob": 2.8409667720552534e-05,
      "seek": 22018,
      "start": 237.02000427246094,
      "temperature": 0.0,
      "text": " because most of the logic is going to be over here.",
      "tokens": [
        51206,
        570,
        881,
        295,
        264,
        9952,
        307,
        516,
        281,
        312,
        670,
        510,
        13,
        51331
      ]
    },
    {
      "id": 76,
      "avg_logprob": -0.16896255314350128,
      "compression_ratio": 1.7035398483276367,
      "end": 242.74000549316406,
      "no_speech_prob": 2.8409667720552534e-05,
      "seek": 22018,
      "start": 241.32000732421875,
      "temperature": 0.0,
      "text": " Going back to the code,",
      "tokens": [
        51421,
        10963,
        646,
        281,
        264,
        3089,
        11,
        51492
      ]
    },
    {
      "id": 77,
      "avg_logprob": -0.16896255314350128,
      "compression_ratio": 1.7035398483276367,
      "end": 245.22000122070312,
      "no_speech_prob": 2.8409667720552534e-05,
      "seek": 22018,
      "start": 242.74000549316406,
      "temperature": 0.0,
      "text": " this is going to ask for a file path,",
      "tokens": [
        51492,
        341,
        307,
        516,
        281,
        1029,
        337,
        257,
        3991,
        3100,
        11,
        51616
      ]
    },
    {
      "id": 78,
      "avg_logprob": -0.16896255314350128,
      "compression_ratio": 1.7035398483276367,
      "end": 248.17999267578125,
      "no_speech_prob": 2.8409667720552534e-05,
      "seek": 22018,
      "start": 245.22000122070312,
      "temperature": 0.0,
      "text": " which is going to be the path to your audio file.",
      "tokens": [
        51616,
        597,
        307,
        516,
        281,
        312,
        264,
        3100,
        281,
        428,
        6278,
        3991,
        13,
        51764
      ]
    },
    {
      "id": 79,
      "avg_logprob": -0.18600519001483917,
      "compression_ratio": 1.7801724672317505,
      "end": 252.6199951171875,
      "no_speech_prob": 1.0129971087735612e-05,
      "seek": 24818,
      "start": 248.17999267578125,
      "temperature": 0.0,
      "text": " After that, it's going to call the transcriptions API",
      "tokens": [
        50364,
        2381,
        300,
        11,
        309,
        311,
        516,
        281,
        818,
        264,
        24444,
        626,
        9362,
        50586
      ]
    },
    {
      "id": 80,
      "avg_logprob": -0.18600519001483917,
      "compression_ratio": 1.7801724672317505,
      "end": 254.66000366210938,
      "no_speech_prob": 1.0129971087735612e-05,
      "seek": 24818,
      "start": 252.6199951171875,
      "temperature": 0.0,
      "text": " with the WhisperOne model",
      "tokens": [
        50586,
        365,
        264,
        41132,
        610,
        15426,
        2316,
        50688
      ]
    },
    {
      "id": 81,
      "avg_logprob": -0.18600519001483917,
      "compression_ratio": 1.7801724672317505,
      "end": 258.55999755859375,
      "no_speech_prob": 1.0129971087735612e-05,
      "seek": 24818,
      "start": 254.66000366210938,
      "temperature": 0.0,
      "text": " to get the transcription with timestamps,",
      "tokens": [
        50688,
        281,
        483,
        264,
        35288,
        365,
        49108,
        23150,
        11,
        50883
      ]
    },
    {
      "id": 82,
      "avg_logprob": -0.18600519001483917,
      "compression_ratio": 1.7801724672317505,
      "end": 262.6600036621094,
      "no_speech_prob": 1.0129971087735612e-05,
      "seek": 24818,
      "start": 258.55999755859375,
      "temperature": 0.0,
      "text": " and you have to give it a response format of verbose JSON.",
      "tokens": [
        50883,
        293,
        291,
        362,
        281,
        976,
        309,
        257,
        4134,
        7877,
        295,
        9595,
        541,
        31828,
        13,
        51088
      ]
    },
    {
      "id": 83,
      "avg_logprob": -0.18600519001483917,
      "compression_ratio": 1.7801724672317505,
      "end": 264.70001220703125,
      "no_speech_prob": 1.0129971087735612e-05,
      "seek": 24818,
      "start": 262.6600036621094,
      "temperature": 0.0,
      "text": " Otherwise, you don't get the timestamp,",
      "tokens": [
        51088,
        10328,
        11,
        291,
        500,
        380,
        483,
        264,
        49108,
        1215,
        11,
        51190
      ]
    },
    {
      "id": 84,
      "avg_logprob": -0.18600519001483917,
      "compression_ratio": 1.7801724672317505,
      "end": 267.05999755859375,
      "no_speech_prob": 1.0129971087735612e-05,
      "seek": 24818,
      "start": 264.70001220703125,
      "temperature": 0.0,
      "text": " you only get the transcript.",
      "tokens": [
        51190,
        291,
        787,
        483,
        264,
        24444,
        13,
        51308
      ]
    },
    {
      "id": 85,
      "avg_logprob": -0.18600519001483917,
      "compression_ratio": 1.7801724672317505,
      "end": 270.67999267578125,
      "no_speech_prob": 1.0129971087735612e-05,
      "seek": 24818,
      "start": 267.05999755859375,
      "temperature": 0.0,
      "text": " Now, after this line, you should have the JSON file,",
      "tokens": [
        51308,
        823,
        11,
        934,
        341,
        1622,
        11,
        291,
        820,
        362,
        264,
        31828,
        3991,
        11,
        51489
      ]
    },
    {
      "id": 86,
      "avg_logprob": -0.18600519001483917,
      "compression_ratio": 1.7801724672317505,
      "end": 273.6199951171875,
      "no_speech_prob": 1.0129971087735612e-05,
      "seek": 24818,
      "start": 270.67999267578125,
      "temperature": 0.0,
      "text": " and we're going to quickly take a look at the JSON file,",
      "tokens": [
        51489,
        293,
        321,
        434,
        516,
        281,
        2661,
        747,
        257,
        574,
        412,
        264,
        31828,
        3991,
        11,
        51636
      ]
    },
    {
      "id": 87,
      "avg_logprob": -0.18600519001483917,
      "compression_ratio": 1.7801724672317505,
      "end": 276.8599853515625,
      "no_speech_prob": 1.0129971087735612e-05,
      "seek": 24818,
      "start": 273.6199951171875,
      "temperature": 0.0,
      "text": " which is going to be here in the generated directory.",
      "tokens": [
        51636,
        597,
        307,
        516,
        281,
        312,
        510,
        294,
        264,
        10833,
        21120,
        13,
        51798
      ]
    },
    {
      "id": 88,
      "avg_logprob": -0.19323420524597168,
      "compression_ratio": 1.6162161827087402,
      "end": 281.2200012207031,
      "no_speech_prob": 4.0694540075492114e-05,
      "seek": 27686,
      "start": 276.8599853515625,
      "temperature": 0.0,
      "text": " If I look at the audio timestamp,",
      "tokens": [
        50364,
        759,
        286,
        574,
        412,
        264,
        6278,
        49108,
        1215,
        11,
        50582
      ]
    },
    {
      "id": 89,
      "avg_logprob": -0.19323420524597168,
      "compression_ratio": 1.6162161827087402,
      "end": 283.3800048828125,
      "no_speech_prob": 4.0694540075492114e-05,
      "seek": 27686,
      "start": 281.2200012207031,
      "temperature": 0.0,
      "text": " you're going to see this is the response",
      "tokens": [
        50582,
        291,
        434,
        516,
        281,
        536,
        341,
        307,
        264,
        4134,
        50690
      ]
    },
    {
      "id": 90,
      "avg_logprob": -0.19323420524597168,
      "compression_ratio": 1.6162161827087402,
      "end": 285.7200012207031,
      "no_speech_prob": 4.0694540075492114e-05,
      "seek": 27686,
      "start": 283.3800048828125,
      "temperature": 0.0,
      "text": " that the Whisper model gives me,",
      "tokens": [
        50690,
        300,
        264,
        41132,
        610,
        2316,
        2709,
        385,
        11,
        50807
      ]
    },
    {
      "id": 91,
      "avg_logprob": -0.19323420524597168,
      "compression_ratio": 1.6162161827087402,
      "end": 290.7200012207031,
      "no_speech_prob": 4.0694540075492114e-05,
      "seek": 27686,
      "start": 285.7200012207031,
      "temperature": 0.0,
      "text": " where for every timestamp,",
      "tokens": [
        50807,
        689,
        337,
        633,
        49108,
        1215,
        11,
        51057
      ]
    },
    {
      "id": 92,
      "avg_logprob": -0.19323420524597168,
      "compression_ratio": 1.6162161827087402,
      "end": 293.4599914550781,
      "no_speech_prob": 4.0694540075492114e-05,
      "seek": 27686,
      "start": 291.1400146484375,
      "temperature": 0.0,
      "text": " it gives me the text, right?",
      "tokens": [
        51078,
        309,
        2709,
        385,
        264,
        2487,
        11,
        558,
        30,
        51194
      ]
    },
    {
      "id": 93,
      "avg_logprob": -0.19323420524597168,
      "compression_ratio": 1.6162161827087402,
      "end": 295.8800048828125,
      "no_speech_prob": 4.0694540075492114e-05,
      "seek": 27686,
      "start": 293.4599914550781,
      "temperature": 0.0,
      "text": " So it uses some arbitrary logic",
      "tokens": [
        51194,
        407,
        309,
        4960,
        512,
        23211,
        9952,
        51315
      ]
    },
    {
      "id": 94,
      "avg_logprob": -0.19323420524597168,
      "compression_ratio": 1.6162161827087402,
      "end": 299.1000061035156,
      "no_speech_prob": 4.0694540075492114e-05,
      "seek": 27686,
      "start": 295.8800048828125,
      "temperature": 0.0,
      "text": " to divide the file into different segments.",
      "tokens": [
        51315,
        281,
        9845,
        264,
        3991,
        666,
        819,
        19904,
        13,
        51476
      ]
    },
    {
      "id": 95,
      "avg_logprob": -0.19323420524597168,
      "compression_ratio": 1.6162161827087402,
      "end": 302.4800109863281,
      "no_speech_prob": 4.0694540075492114e-05,
      "seek": 27686,
      "start": 299.1000061035156,
      "temperature": 0.0,
      "text": " For each segment, it's going to give me the start timestamp",
      "tokens": [
        51476,
        1171,
        1184,
        9469,
        11,
        309,
        311,
        516,
        281,
        976,
        385,
        264,
        722,
        49108,
        1215,
        51645
      ]
    },
    {
      "id": 96,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 305.0400085449219,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 302.4800109863281,
      "temperature": 0.0,
      "text": " and how long the segment is for,",
      "tokens": [
        50364,
        293,
        577,
        938,
        264,
        9469,
        307,
        337,
        11,
        50492
      ]
    },
    {
      "id": 97,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 307.67999267578125,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 305.0400085449219,
      "temperature": 0.0,
      "text": " and the text over here, okay?",
      "tokens": [
        50492,
        293,
        264,
        2487,
        670,
        510,
        11,
        1392,
        30,
        50624
      ]
    },
    {
      "id": 98,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 308.7799987792969,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 307.67999267578125,
      "temperature": 0.0,
      "text": " So if you go through it,",
      "tokens": [
        50624,
        407,
        498,
        291,
        352,
        807,
        309,
        11,
        50679
      ]
    },
    {
      "id": 99,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 310.760009765625,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 308.7799987792969,
      "temperature": 0.0,
      "text": " you can see the text has been divided",
      "tokens": [
        50679,
        291,
        393,
        536,
        264,
        2487,
        575,
        668,
        6666,
        50778
      ]
    },
    {
      "id": 100,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 312.260009765625,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 310.760009765625,
      "temperature": 0.0,
      "text": " into multiple segments,",
      "tokens": [
        50778,
        666,
        3866,
        19904,
        11,
        50853
      ]
    },
    {
      "id": 101,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 316.5400085449219,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 312.260009765625,
      "temperature": 0.0,
      "text": " and if I go all the way to the end,",
      "tokens": [
        50853,
        293,
        498,
        286,
        352,
        439,
        264,
        636,
        281,
        264,
        917,
        11,
        51067
      ]
    },
    {
      "id": 102,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 320.44000244140625,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 317.4800109863281,
      "temperature": 0.0,
      "text": " you're going to see that I have the end of the video",
      "tokens": [
        51114,
        291,
        434,
        516,
        281,
        536,
        300,
        286,
        362,
        264,
        917,
        295,
        264,
        960,
        51262
      ]
    },
    {
      "id": 103,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 323.0,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 320.44000244140625,
      "temperature": 0.0,
      "text": " where I say, take care, bye-bye, okay?",
      "tokens": [
        51262,
        689,
        286,
        584,
        11,
        747,
        1127,
        11,
        6543,
        12,
        6650,
        11,
        1392,
        30,
        51390
      ]
    },
    {
      "id": 104,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 324.67999267578125,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 323.0,
      "temperature": 0.0,
      "text": " So this is how the file looks like",
      "tokens": [
        51390,
        407,
        341,
        307,
        577,
        264,
        3991,
        1542,
        411,
        51474
      ]
    },
    {
      "id": 105,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 329.0799865722656,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 324.67999267578125,
      "temperature": 0.0,
      "text": " when you get it from OpenAI at this step.",
      "tokens": [
        51474,
        562,
        291,
        483,
        309,
        490,
        7238,
        48698,
        412,
        341,
        1823,
        13,
        51694
      ]
    },
    {
      "id": 106,
      "avg_logprob": -0.21011178195476532,
      "compression_ratio": 1.6581196784973145,
      "end": 332.20001220703125,
      "no_speech_prob": 6.401981954695657e-05,
      "seek": 30248,
      "start": 329.0799865722656,
      "temperature": 0.0,
      "text": " Now, once we have the transcript,",
      "tokens": [
        51694,
        823,
        11,
        1564,
        321,
        362,
        264,
        24444,
        11,
        51850
      ]
    },
    {
      "id": 107,
      "avg_logprob": -0.1868680864572525,
      "compression_ratio": 1.708154559135437,
      "end": 334.67999267578125,
      "no_speech_prob": 0.00011959812400164083,
      "seek": 33220,
      "start": 332.20001220703125,
      "temperature": 0.0,
      "text": " you're going to see that this is the exact text",
      "tokens": [
        50364,
        291,
        434,
        516,
        281,
        536,
        300,
        341,
        307,
        264,
        1900,
        2487,
        50488
      ]
    },
    {
      "id": 108,
      "avg_logprob": -0.1868680864572525,
      "compression_ratio": 1.708154559135437,
      "end": 337.0799865722656,
      "no_speech_prob": 0.00011959812400164083,
      "seek": 33220,
      "start": 334.67999267578125,
      "temperature": 0.0,
      "text": " or the exact words I say in the video.",
      "tokens": [
        50488,
        420,
        264,
        1900,
        2283,
        286,
        584,
        294,
        264,
        960,
        13,
        50608
      ]
    },
    {
      "id": 109,
      "avg_logprob": -0.1868680864572525,
      "compression_ratio": 1.708154559135437,
      "end": 338.79998779296875,
      "no_speech_prob": 0.00011959812400164083,
      "seek": 33220,
      "start": 337.0799865722656,
      "temperature": 0.0,
      "text": " So even though we have the timestamp,",
      "tokens": [
        50608,
        407,
        754,
        1673,
        321,
        362,
        264,
        49108,
        1215,
        11,
        50694
      ]
    },
    {
      "id": 110,
      "avg_logprob": -0.1868680864572525,
      "compression_ratio": 1.708154559135437,
      "end": 341.239990234375,
      "no_speech_prob": 0.00011959812400164083,
      "seek": 33220,
      "start": 338.79998779296875,
      "temperature": 0.0,
      "text": " we don't want to just upload this",
      "tokens": [
        50694,
        321,
        500,
        380,
        528,
        281,
        445,
        6580,
        341,
        50816
      ]
    },
    {
      "id": 111,
      "avg_logprob": -0.1868680864572525,
      "compression_ratio": 1.708154559135437,
      "end": 343.79998779296875,
      "no_speech_prob": 0.00011959812400164083,
      "seek": 33220,
      "start": 341.239990234375,
      "temperature": 0.0,
      "text": " into YouTube as chapters.",
      "tokens": [
        50816,
        666,
        3088,
        382,
        20013,
        13,
        50944
      ]
    },
    {
      "id": 112,
      "avg_logprob": -0.1868680864572525,
      "compression_ratio": 1.708154559135437,
      "end": 347.55999755859375,
      "no_speech_prob": 0.00011959812400164083,
      "seek": 33220,
      "start": 343.79998779296875,
      "temperature": 0.0,
      "text": " Instead, we want to pass the transcript back to the model",
      "tokens": [
        50944,
        7156,
        11,
        321,
        528,
        281,
        1320,
        264,
        24444,
        646,
        281,
        264,
        2316,
        51132
      ]
    },
    {
      "id": 113,
      "avg_logprob": -0.1868680864572525,
      "compression_ratio": 1.708154559135437,
      "end": 351.17999267578125,
      "no_speech_prob": 0.00011959812400164083,
      "seek": 33220,
      "start": 347.55999755859375,
      "temperature": 0.0,
      "text": " and tell the model to generate a summary with timestamps.",
      "tokens": [
        51132,
        293,
        980,
        264,
        2316,
        281,
        8460,
        257,
        12691,
        365,
        49108,
        23150,
        13,
        51313
      ]
    },
    {
      "id": 114,
      "avg_logprob": -0.1868680864572525,
      "compression_ratio": 1.708154559135437,
      "end": 353.739990234375,
      "no_speech_prob": 0.00011959812400164083,
      "seek": 33220,
      "start": 351.17999267578125,
      "temperature": 0.0,
      "text": " Now, let's see how I want to do that here.",
      "tokens": [
        51313,
        823,
        11,
        718,
        311,
        536,
        577,
        286,
        528,
        281,
        360,
        300,
        510,
        13,
        51441
      ]
    },
    {
      "id": 115,
      "avg_logprob": -0.1868680864572525,
      "compression_ratio": 1.708154559135437,
      "end": 360.3599853515625,
      "no_speech_prob": 0.00011959812400164083,
      "seek": 33220,
      "start": 355.3599853515625,
      "temperature": 0.0,
      "text": " So you can see that once we have the transcribed audio",
      "tokens": [
        51522,
        407,
        291,
        393,
        536,
        300,
        1564,
        321,
        362,
        264,
        1145,
        18732,
        6278,
        51772
      ]
    },
    {
      "id": 116,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 363.55999755859375,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 360.4800109863281,
      "temperature": 0.0,
      "text": " or the transcription with timestamp,",
      "tokens": [
        50370,
        420,
        264,
        35288,
        365,
        49108,
        1215,
        11,
        50524
      ]
    },
    {
      "id": 117,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 368.55999755859375,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 363.55999755859375,
      "temperature": 0.0,
      "text": " we call a function over here,",
      "tokens": [
        50524,
        321,
        818,
        257,
        2445,
        670,
        510,
        11,
        50774
      ]
    },
    {
      "id": 118,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 370.9200134277344,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 368.6000061035156,
      "temperature": 0.0,
      "text": " generate chapters from transcript.",
      "tokens": [
        50776,
        8460,
        20013,
        490,
        24444,
        13,
        50892
      ]
    },
    {
      "id": 119,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 373.1600036621094,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 370.9200134277344,
      "temperature": 0.0,
      "text": " So let's go ahead and take a look at that.",
      "tokens": [
        50892,
        407,
        718,
        311,
        352,
        2286,
        293,
        747,
        257,
        574,
        412,
        300,
        13,
        51004
      ]
    },
    {
      "id": 120,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 375.20001220703125,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 374.1400146484375,
      "temperature": 0.0,
      "text": " That is the function here.",
      "tokens": [
        51053,
        663,
        307,
        264,
        2445,
        510,
        13,
        51106
      ]
    },
    {
      "id": 121,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 376.55999755859375,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 375.20001220703125,
      "temperature": 0.0,
      "text": " Again, this is going to be linked",
      "tokens": [
        51106,
        3764,
        11,
        341,
        307,
        516,
        281,
        312,
        9408,
        51174
      ]
    },
    {
      "id": 122,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 378.1199951171875,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 376.55999755859375,
      "temperature": 0.0,
      "text": " in the description below",
      "tokens": [
        51174,
        294,
        264,
        3855,
        2507,
        51252
      ]
    },
    {
      "id": 123,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 380.760009765625,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 378.1199951171875,
      "temperature": 0.0,
      "text": " if you want to take a look at it yourself.",
      "tokens": [
        51252,
        498,
        291,
        528,
        281,
        747,
        257,
        574,
        412,
        309,
        1803,
        13,
        51384
      ]
    },
    {
      "id": 124,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 385.0799865722656,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 380.760009765625,
      "temperature": 0.0,
      "text": " But all this one does is it opens the JSON file",
      "tokens": [
        51384,
        583,
        439,
        341,
        472,
        775,
        307,
        309,
        9870,
        264,
        31828,
        3991,
        51600
      ]
    },
    {
      "id": 125,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 386.3800048828125,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 385.0799865722656,
      "temperature": 0.0,
      "text": " that I just showed you,",
      "tokens": [
        51600,
        300,
        286,
        445,
        4712,
        291,
        11,
        51665
      ]
    },
    {
      "id": 126,
      "avg_logprob": -0.2105998396873474,
      "compression_ratio": 1.643478274345398,
      "end": 389.0,
      "no_speech_prob": 3.647833364084363e-05,
      "seek": 36036,
      "start": 387.260009765625,
      "temperature": 0.0,
      "text": " and then it does some formatting",
      "tokens": [
        51709,
        293,
        550,
        309,
        775,
        512,
        39366,
        51796
      ]
    },
    {
      "id": 127,
      "avg_logprob": -0.24968791007995605,
      "compression_ratio": 1.7810651063919067,
      "end": 394.0,
      "no_speech_prob": 9.915198461385444e-05,
      "seek": 38900,
      "start": 389.0,
      "temperature": 0.0,
      "text": " to make sure both the starting and the starting timestamp",
      "tokens": [
        50364,
        281,
        652,
        988,
        1293,
        264,
        2891,
        293,
        264,
        2891,
        49108,
        1215,
        50614
      ]
    },
    {
      "id": 128,
      "avg_logprob": -0.24968791007995605,
      "compression_ratio": 1.7810651063919067,
      "end": 395.6000061035156,
      "no_speech_prob": 9.915198461385444e-05,
      "seek": 38900,
      "start": 394.0,
      "temperature": 0.0,
      "text": " is in minutes and seconds,",
      "tokens": [
        50614,
        307,
        294,
        2077,
        293,
        3949,
        11,
        50694
      ]
    },
    {
      "id": 129,
      "avg_logprob": -0.24968791007995605,
      "compression_ratio": 1.7810651063919067,
      "end": 400.2200012207031,
      "no_speech_prob": 9.915198461385444e-05,
      "seek": 38900,
      "start": 395.6000061035156,
      "temperature": 0.0,
      "text": " or in other words, in, let me show you,",
      "tokens": [
        50694,
        420,
        294,
        661,
        2283,
        11,
        294,
        11,
        718,
        385,
        855,
        291,
        11,
        50925
      ]
    },
    {
      "id": 130,
      "avg_logprob": -0.24968791007995605,
      "compression_ratio": 1.7810651063919067,
      "end": 402.70001220703125,
      "no_speech_prob": 9.915198461385444e-05,
      "seek": 38900,
      "start": 400.2200012207031,
      "temperature": 0.0,
      "text": " in a format like this, okay?",
      "tokens": [
        50925,
        294,
        257,
        7877,
        411,
        341,
        11,
        1392,
        30,
        51049
      ]
    },
    {
      "id": 131,
      "avg_logprob": -0.24968791007995605,
      "compression_ratio": 1.7810651063919067,
      "end": 408.67999267578125,
      "no_speech_prob": 9.915198461385444e-05,
      "seek": 38900,
      "start": 404.05999755859375,
      "temperature": 0.0,
      "text": " And the text, it parses out the text from,",
      "tokens": [
        51117,
        400,
        264,
        2487,
        11,
        309,
        21156,
        279,
        484,
        264,
        2487,
        490,
        11,
        51348
      ]
    },
    {
      "id": 132,
      "avg_logprob": -0.24968791007995605,
      "compression_ratio": 1.7810651063919067,
      "end": 411.5,
      "no_speech_prob": 9.915198461385444e-05,
      "seek": 38900,
      "start": 408.67999267578125,
      "temperature": 0.0,
      "text": " it parses out the text from here, okay?",
      "tokens": [
        51348,
        309,
        21156,
        279,
        484,
        264,
        2487,
        490,
        510,
        11,
        1392,
        30,
        51489
      ]
    },
    {
      "id": 133,
      "avg_logprob": -0.24968791007995605,
      "compression_ratio": 1.7810651063919067,
      "end": 414.8800048828125,
      "no_speech_prob": 9.915198461385444e-05,
      "seek": 38900,
      "start": 412.5199890136719,
      "temperature": 0.0,
      "text": " And then it appends it to a list.",
      "tokens": [
        51540,
        400,
        550,
        309,
        724,
        2581,
        309,
        281,
        257,
        1329,
        13,
        51658
      ]
    },
    {
      "id": 134,
      "avg_logprob": -0.24968791007995605,
      "compression_ratio": 1.7810651063919067,
      "end": 417.79998779296875,
      "no_speech_prob": 9.915198461385444e-05,
      "seek": 38900,
      "start": 414.8800048828125,
      "temperature": 0.0,
      "text": " So at the end of the for loop,",
      "tokens": [
        51658,
        407,
        412,
        264,
        917,
        295,
        264,
        337,
        6367,
        11,
        51804
      ]
    },
    {
      "id": 135,
      "avg_logprob": -0.20700353384017944,
      "compression_ratio": 1.6635944843292236,
      "end": 420.44000244140625,
      "no_speech_prob": 2.8409678634488955e-05,
      "seek": 41780,
      "start": 417.79998779296875,
      "temperature": 0.0,
      "text": " you end up with an array of summary lines",
      "tokens": [
        50364,
        291,
        917,
        493,
        365,
        364,
        10225,
        295,
        12691,
        3876,
        50496
      ]
    },
    {
      "id": 136,
      "avg_logprob": -0.20700353384017944,
      "compression_ratio": 1.6635944843292236,
      "end": 424.9800109863281,
      "no_speech_prob": 2.8409678634488955e-05,
      "seek": 41780,
      "start": 420.44000244140625,
      "temperature": 0.0,
      "text": " where each line is the timestamp in minute colon second,",
      "tokens": [
        50496,
        689,
        1184,
        1622,
        307,
        264,
        49108,
        1215,
        294,
        3456,
        8255,
        1150,
        11,
        50723
      ]
    },
    {
      "id": 137,
      "avg_logprob": -0.20700353384017944,
      "compression_ratio": 1.6635944843292236,
      "end": 429.20001220703125,
      "no_speech_prob": 2.8409678634488955e-05,
      "seek": 41780,
      "start": 424.9800109863281,
      "temperature": 0.0,
      "text": " and then the full text of what I said in the video, okay?",
      "tokens": [
        50723,
        293,
        550,
        264,
        1577,
        2487,
        295,
        437,
        286,
        848,
        294,
        264,
        960,
        11,
        1392,
        30,
        50934
      ]
    },
    {
      "id": 138,
      "avg_logprob": -0.20700353384017944,
      "compression_ratio": 1.6635944843292236,
      "end": 430.9200134277344,
      "no_speech_prob": 2.8409678634488955e-05,
      "seek": 41780,
      "start": 429.20001220703125,
      "temperature": 0.0,
      "text": " Now we're going to pass this to the model.",
      "tokens": [
        50934,
        823,
        321,
        434,
        516,
        281,
        1320,
        341,
        281,
        264,
        2316,
        13,
        51020
      ]
    },
    {
      "id": 139,
      "avg_logprob": -0.20700353384017944,
      "compression_ratio": 1.6635944843292236,
      "end": 435.9200134277344,
      "no_speech_prob": 2.8409678634488955e-05,
      "seek": 41780,
      "start": 430.9200134277344,
      "temperature": 0.0,
      "text": " You're going to see, I pass it to the model over here, okay?",
      "tokens": [
        51020,
        509,
        434,
        516,
        281,
        536,
        11,
        286,
        1320,
        309,
        281,
        264,
        2316,
        670,
        510,
        11,
        1392,
        30,
        51270
      ]
    },
    {
      "id": 140,
      "avg_logprob": -0.20700353384017944,
      "compression_ratio": 1.6635944843292236,
      "end": 441.44000244140625,
      "no_speech_prob": 2.8409678634488955e-05,
      "seek": 41780,
      "start": 436.44000244140625,
      "temperature": 0.0,
      "text": " Now I do prefix the summary with this prompt,",
      "tokens": [
        51296,
        823,
        286,
        360,
        46969,
        264,
        12691,
        365,
        341,
        12391,
        11,
        51546
      ]
    },
    {
      "id": 141,
      "avg_logprob": -0.20700353384017944,
      "compression_ratio": 1.6635944843292236,
      "end": 446.3999938964844,
      "no_speech_prob": 2.8409678634488955e-05,
      "seek": 41780,
      "start": 442.5,
      "temperature": 0.0,
      "text": " which just says given the following transcript segment",
      "tokens": [
        51599,
        597,
        445,
        1619,
        2212,
        264,
        3480,
        24444,
        9469,
        51794
      ]
    },
    {
      "id": 142,
      "avg_logprob": -0.23975597321987152,
      "compression_ratio": 1.620192289352417,
      "end": 450.6000061035156,
      "no_speech_prob": 0.0003053477266803384,
      "seek": 44640,
      "start": 446.4800109863281,
      "temperature": 0.0,
      "text": " with timestamp, generate YouTube chapters in the format,",
      "tokens": [
        50368,
        365,
        49108,
        1215,
        11,
        8460,
        3088,
        20013,
        294,
        264,
        7877,
        11,
        50574
      ]
    },
    {
      "id": 143,
      "avg_logprob": -0.23975597321987152,
      "compression_ratio": 1.620192289352417,
      "end": 452.4800109863281,
      "no_speech_prob": 0.0003053477266803384,
      "seek": 44640,
      "start": 450.6000061035156,
      "temperature": 0.0,
      "text": " and I give it the format here,",
      "tokens": [
        50574,
        293,
        286,
        976,
        309,
        264,
        7877,
        510,
        11,
        50668
      ]
    },
    {
      "id": 144,
      "avg_logprob": -0.23975597321987152,
      "compression_ratio": 1.620192289352417,
      "end": 456.1199951171875,
      "no_speech_prob": 0.0003053477266803384,
      "seek": 44640,
      "start": 452.4800109863281,
      "temperature": 0.0,
      "text": " and I tell it to only output the chapter list, nothing else.",
      "tokens": [
        50668,
        293,
        286,
        980,
        309,
        281,
        787,
        5598,
        264,
        7187,
        1329,
        11,
        1825,
        1646,
        13,
        50850
      ]
    },
    {
      "id": 145,
      "avg_logprob": -0.23975597321987152,
      "compression_ratio": 1.620192289352417,
      "end": 459.8599853515625,
      "no_speech_prob": 0.0003053477266803384,
      "seek": 44640,
      "start": 456.1199951171875,
      "temperature": 0.0,
      "text": " And then the summary goes here, okay?",
      "tokens": [
        50850,
        400,
        550,
        264,
        12691,
        1709,
        510,
        11,
        1392,
        30,
        51037
      ]
    },
    {
      "id": 146,
      "avg_logprob": -0.23975597321987152,
      "compression_ratio": 1.620192289352417,
      "end": 465.0400085449219,
      "no_speech_prob": 0.0003053477266803384,
      "seek": 44640,
      "start": 461.1600036621094,
      "temperature": 0.0,
      "text": " Once we have the prompt, we pass it to our 4.0 model",
      "tokens": [
        51102,
        3443,
        321,
        362,
        264,
        12391,
        11,
        321,
        1320,
        309,
        281,
        527,
        1017,
        13,
        15,
        2316,
        51296
      ]
    },
    {
      "id": 147,
      "avg_logprob": -0.23975597321987152,
      "compression_ratio": 1.620192289352417,
      "end": 468.7200012207031,
      "no_speech_prob": 0.0003053477266803384,
      "seek": 44640,
      "start": 465.9599914550781,
      "temperature": 0.0,
      "text": " and give it a system prompt as well.",
      "tokens": [
        51342,
        293,
        976,
        309,
        257,
        1185,
        12391,
        382,
        731,
        13,
        51480
      ]
    },
    {
      "id": 148,
      "avg_logprob": -0.23975597321987152,
      "compression_ratio": 1.620192289352417,
      "end": 471.3999938964844,
      "no_speech_prob": 0.0003053477266803384,
      "seek": 44640,
      "start": 469.9200134277344,
      "temperature": 0.0,
      "text": " The model takes a while,",
      "tokens": [
        51540,
        440,
        2316,
        2516,
        257,
        1339,
        11,
        51614
      ]
    },
    {
      "id": 149,
      "avg_logprob": -0.23975597321987152,
      "compression_ratio": 1.620192289352417,
      "end": 474.4800109863281,
      "no_speech_prob": 0.0003053477266803384,
      "seek": 44640,
      "start": 471.3999938964844,
      "temperature": 0.0,
      "text": " but then it spits out the response,",
      "tokens": [
        51614,
        457,
        550,
        309,
        637,
        1208,
        484,
        264,
        4134,
        11,
        51768
      ]
    },
    {
      "id": 150,
      "avg_logprob": -0.18179696798324585,
      "compression_ratio": 1.6631579399108887,
      "end": 478.2799987792969,
      "no_speech_prob": 0.00011061101395171136,
      "seek": 47448,
      "start": 474.4800109863281,
      "temperature": 0.0,
      "text": " which we save in a chapters.txt file.",
      "tokens": [
        50364,
        597,
        321,
        3155,
        294,
        257,
        20013,
        13,
        83,
        734,
        3991,
        13,
        50554
      ]
    },
    {
      "id": 151,
      "avg_logprob": -0.18179696798324585,
      "compression_ratio": 1.6631579399108887,
      "end": 482.1600036621094,
      "no_speech_prob": 0.00011061101395171136,
      "seek": 47448,
      "start": 478.2799987792969,
      "temperature": 0.0,
      "text": " And you're going to see chapters.txt file here.",
      "tokens": [
        50554,
        400,
        291,
        434,
        516,
        281,
        536,
        20013,
        13,
        83,
        734,
        3991,
        510,
        13,
        50748
      ]
    },
    {
      "id": 152,
      "avg_logprob": -0.18179696798324585,
      "compression_ratio": 1.6631579399108887,
      "end": 483.2799987792969,
      "no_speech_prob": 0.00011061101395171136,
      "seek": 47448,
      "start": 482.1600036621094,
      "temperature": 0.0,
      "text": " There you go.",
      "tokens": [
        50748,
        821,
        291,
        352,
        13,
        50804
      ]
    },
    {
      "id": 153,
      "avg_logprob": -0.18179696798324585,
      "compression_ratio": 1.6631579399108887,
      "end": 487.0400085449219,
      "no_speech_prob": 0.00011061101395171136,
      "seek": 47448,
      "start": 483.2799987792969,
      "temperature": 0.0,
      "text": " And I just finished running the program,",
      "tokens": [
        50804,
        400,
        286,
        445,
        4335,
        2614,
        264,
        1461,
        11,
        50992
      ]
    },
    {
      "id": 154,
      "avg_logprob": -0.18179696798324585,
      "compression_ratio": 1.6631579399108887,
      "end": 491.7200012207031,
      "no_speech_prob": 0.00011061101395171136,
      "seek": 47448,
      "start": 487.0400085449219,
      "temperature": 0.0,
      "text": " and you're going to see when I initially run it,",
      "tokens": [
        50992,
        293,
        291,
        434,
        516,
        281,
        536,
        562,
        286,
        9105,
        1190,
        309,
        11,
        51226
      ]
    },
    {
      "id": 155,
      "avg_logprob": -0.18179696798324585,
      "compression_ratio": 1.6631579399108887,
      "end": 495.0799865722656,
      "no_speech_prob": 0.00011061101395171136,
      "seek": 47448,
      "start": 491.7200012207031,
      "temperature": 0.0,
      "text": " this is how it generates that transcript.",
      "tokens": [
        51226,
        341,
        307,
        577,
        309,
        23815,
        300,
        24444,
        13,
        51394
      ]
    },
    {
      "id": 156,
      "avg_logprob": -0.18179696798324585,
      "compression_ratio": 1.6631579399108887,
      "end": 497.4800109863281,
      "no_speech_prob": 0.00011061101395171136,
      "seek": 47448,
      "start": 495.0799865722656,
      "temperature": 0.0,
      "text": " So you can see the timestamp",
      "tokens": [
        51394,
        407,
        291,
        393,
        536,
        264,
        49108,
        1215,
        51514
      ]
    },
    {
      "id": 157,
      "avg_logprob": -0.18179696798324585,
      "compression_ratio": 1.6631579399108887,
      "end": 499.94000244140625,
      "no_speech_prob": 0.00011061101395171136,
      "seek": 47448,
      "start": 497.4800109863281,
      "temperature": 0.0,
      "text": " and then what I said in the video.",
      "tokens": [
        51514,
        293,
        550,
        437,
        286,
        848,
        294,
        264,
        960,
        13,
        51637
      ]
    },
    {
      "id": 158,
      "avg_logprob": -0.18179696798324585,
      "compression_ratio": 1.6631579399108887,
      "end": 501.3399963378906,
      "no_speech_prob": 0.00011061101395171136,
      "seek": 47448,
      "start": 499.94000244140625,
      "temperature": 0.0,
      "text": " And then at the end,",
      "tokens": [
        51637,
        400,
        550,
        412,
        264,
        917,
        11,
        51707
      ]
    },
    {
      "id": 159,
      "avg_logprob": -0.17901401221752167,
      "compression_ratio": 1.7203389406204224,
      "end": 506.3399963378906,
      "no_speech_prob": 3.426829061936587e-05,
      "seek": 50134,
      "start": 501.3399963378906,
      "temperature": 0.0,
      "text": " I end up with the chapters that is saved in the file here.",
      "tokens": [
        50364,
        286,
        917,
        493,
        365,
        264,
        20013,
        300,
        307,
        6624,
        294,
        264,
        3991,
        510,
        13,
        50614
      ]
    },
    {
      "id": 160,
      "avg_logprob": -0.17901401221752167,
      "compression_ratio": 1.7203389406204224,
      "end": 510.82000732421875,
      "no_speech_prob": 3.426829061936587e-05,
      "seek": 50134,
      "start": 506.6199951171875,
      "temperature": 0.0,
      "text": " And once you look at the file, this is the transcript.",
      "tokens": [
        50628,
        400,
        1564,
        291,
        574,
        412,
        264,
        3991,
        11,
        341,
        307,
        264,
        24444,
        13,
        50838
      ]
    },
    {
      "id": 161,
      "avg_logprob": -0.17901401221752167,
      "compression_ratio": 1.7203389406204224,
      "end": 514.219970703125,
      "no_speech_prob": 3.426829061936587e-05,
      "seek": 50134,
      "start": 510.82000732421875,
      "temperature": 0.0,
      "text": " So if you take a look at my last three or four videos,",
      "tokens": [
        50838,
        407,
        498,
        291,
        747,
        257,
        574,
        412,
        452,
        1036,
        1045,
        420,
        1451,
        2145,
        11,
        51008
      ]
    },
    {
      "id": 162,
      "avg_logprob": -0.17901401221752167,
      "compression_ratio": 1.7203389406204224,
      "end": 517.3800048828125,
      "no_speech_prob": 3.426829061936587e-05,
      "seek": 50134,
      "start": 514.219970703125,
      "temperature": 0.0,
      "text": " you're going to see YouTube chapters for each of them.",
      "tokens": [
        51008,
        291,
        434,
        516,
        281,
        536,
        3088,
        20013,
        337,
        1184,
        295,
        552,
        13,
        51166
      ]
    },
    {
      "id": 163,
      "avg_logprob": -0.17901401221752167,
      "compression_ratio": 1.7203389406204224,
      "end": 519.1400146484375,
      "no_speech_prob": 3.426829061936587e-05,
      "seek": 50134,
      "start": 517.3800048828125,
      "temperature": 0.0,
      "text": " And for each of them,",
      "tokens": [
        51166,
        400,
        337,
        1184,
        295,
        552,
        11,
        51254
      ]
    },
    {
      "id": 164,
      "avg_logprob": -0.17901401221752167,
      "compression_ratio": 1.7203389406204224,
      "end": 522.4000244140625,
      "no_speech_prob": 3.426829061936587e-05,
      "seek": 50134,
      "start": 519.1400146484375,
      "temperature": 0.0,
      "text": " the chapters were generated using the program",
      "tokens": [
        51254,
        264,
        20013,
        645,
        10833,
        1228,
        264,
        1461,
        51417
      ]
    },
    {
      "id": 165,
      "avg_logprob": -0.17901401221752167,
      "compression_ratio": 1.7203389406204224,
      "end": 523.6799926757812,
      "no_speech_prob": 3.426829061936587e-05,
      "seek": 50134,
      "start": 522.4000244140625,
      "temperature": 0.0,
      "text": " that you see over here.",
      "tokens": [
        51417,
        300,
        291,
        536,
        670,
        510,
        13,
        51481
      ]
    },
    {
      "id": 166,
      "avg_logprob": -0.17901401221752167,
      "compression_ratio": 1.7203389406204224,
      "end": 526.6199951171875,
      "no_speech_prob": 3.426829061936587e-05,
      "seek": 50134,
      "start": 525.0599975585938,
      "temperature": 0.0,
      "text": " Now let's do a quick recap",
      "tokens": [
        51550,
        823,
        718,
        311,
        360,
        257,
        1702,
        20928,
        51628
      ]
    },
    {
      "id": 167,
      "avg_logprob": -0.17901401221752167,
      "compression_ratio": 1.7203389406204224,
      "end": 529.1799926757812,
      "no_speech_prob": 3.426829061936587e-05,
      "seek": 50134,
      "start": 526.6199951171875,
      "temperature": 0.0,
      "text": " by going back to the system diagram,",
      "tokens": [
        51628,
        538,
        516,
        646,
        281,
        264,
        1185,
        10686,
        11,
        51756
      ]
    },
    {
      "id": 168,
      "avg_logprob": -0.17901401221752167,
      "compression_ratio": 1.7203389406204224,
      "end": 530.7000122070312,
      "no_speech_prob": 3.426829061936587e-05,
      "seek": 50134,
      "start": 529.1799926757812,
      "temperature": 0.0,
      "text": " which is going to be here.",
      "tokens": [
        51756,
        597,
        307,
        516,
        281,
        312,
        510,
        13,
        51832
      ]
    },
    {
      "id": 169,
      "avg_logprob": -0.22090564668178558,
      "compression_ratio": 1.586363673210144,
      "end": 533.9000244140625,
      "no_speech_prob": 0.00035143463173881173,
      "seek": 53070,
      "start": 530.739990234375,
      "temperature": 0.0,
      "text": " Going from left to right, we have our video file.",
      "tokens": [
        50366,
        10963,
        490,
        1411,
        281,
        558,
        11,
        321,
        362,
        527,
        960,
        3991,
        13,
        50524
      ]
    },
    {
      "id": 170,
      "avg_logprob": -0.22090564668178558,
      "compression_ratio": 1.586363673210144,
      "end": 536.0999755859375,
      "no_speech_prob": 0.00035143463173881173,
      "seek": 53070,
      "start": 533.9000244140625,
      "temperature": 0.0,
      "text": " We convert it into audio.",
      "tokens": [
        50524,
        492,
        7620,
        309,
        666,
        6278,
        13,
        50634
      ]
    },
    {
      "id": 171,
      "avg_logprob": -0.22090564668178558,
      "compression_ratio": 1.586363673210144,
      "end": 539.0599975585938,
      "no_speech_prob": 0.00035143463173881173,
      "seek": 53070,
      "start": 536.0999755859375,
      "temperature": 0.0,
      "text": " We feed the audio to the Whisper model.",
      "tokens": [
        50634,
        492,
        3154,
        264,
        6278,
        281,
        264,
        41132,
        610,
        2316,
        13,
        50782
      ]
    },
    {
      "id": 172,
      "avg_logprob": -0.22090564668178558,
      "compression_ratio": 1.586363673210144,
      "end": 542.7000122070312,
      "no_speech_prob": 0.00035143463173881173,
      "seek": 53070,
      "start": 539.0599975585938,
      "temperature": 0.0,
      "text": " The Whisper model generates a JSON file",
      "tokens": [
        50782,
        440,
        41132,
        610,
        2316,
        23815,
        257,
        31828,
        3991,
        50964
      ]
    },
    {
      "id": 173,
      "avg_logprob": -0.22090564668178558,
      "compression_ratio": 1.586363673210144,
      "end": 545.9000244140625,
      "no_speech_prob": 0.00035143463173881173,
      "seek": 53070,
      "start": 542.7000122070312,
      "temperature": 0.0,
      "text": " with the transcript and timestamp.",
      "tokens": [
        50964,
        365,
        264,
        24444,
        293,
        49108,
        1215,
        13,
        51124
      ]
    },
    {
      "id": 174,
      "avg_logprob": -0.22090564668178558,
      "compression_ratio": 1.586363673210144,
      "end": 549.9000244140625,
      "no_speech_prob": 0.00035143463173881173,
      "seek": 53070,
      "start": 545.9000244140625,
      "temperature": 0.0,
      "text": " We take that file and pass it to a different model,",
      "tokens": [
        51124,
        492,
        747,
        300,
        3991,
        293,
        1320,
        309,
        281,
        257,
        819,
        2316,
        11,
        51324
      ]
    },
    {
      "id": 175,
      "avg_logprob": -0.22090564668178558,
      "compression_ratio": 1.586363673210144,
      "end": 553.0599975585938,
      "no_speech_prob": 0.00035143463173881173,
      "seek": 53070,
      "start": 549.9000244140625,
      "temperature": 0.0,
      "text": " GPT-40, with a prompt that tells it",
      "tokens": [
        51324,
        26039,
        51,
        12,
        5254,
        11,
        365,
        257,
        12391,
        300,
        5112,
        309,
        51482
      ]
    },
    {
      "id": 176,
      "avg_logprob": -0.22090564668178558,
      "compression_ratio": 1.586363673210144,
      "end": 554.8599853515625,
      "no_speech_prob": 0.00035143463173881173,
      "seek": 53070,
      "start": 553.0599975585938,
      "temperature": 0.0,
      "text": " to use the full transcript",
      "tokens": [
        51482,
        281,
        764,
        264,
        1577,
        24444,
        51572
      ]
    },
    {
      "id": 177,
      "avg_logprob": -0.22090564668178558,
      "compression_ratio": 1.586363673210144,
      "end": 559.02001953125,
      "no_speech_prob": 0.00035143463173881173,
      "seek": 53070,
      "start": 554.8599853515625,
      "temperature": 0.0,
      "text": " to generate YouTube summary in this format.",
      "tokens": [
        51572,
        281,
        8460,
        3088,
        12691,
        294,
        341,
        7877,
        13,
        51780
      ]
    },
    {
      "id": 178,
      "avg_logprob": -0.20296575129032135,
      "compression_ratio": 1.5650407075881958,
      "end": 564.02001953125,
      "no_speech_prob": 0.00047283980529755354,
      "seek": 55902,
      "start": 559.02001953125,
      "temperature": 0.0,
      "text": " It takes a while, but then the model returns a text object",
      "tokens": [
        50364,
        467,
        2516,
        257,
        1339,
        11,
        457,
        550,
        264,
        2316,
        11247,
        257,
        2487,
        2657,
        50614
      ]
    },
    {
      "id": 179,
      "avg_logprob": -0.20296575129032135,
      "compression_ratio": 1.5650407075881958,
      "end": 567.97998046875,
      "no_speech_prob": 0.00047283980529755354,
      "seek": 55902,
      "start": 564.1400146484375,
      "temperature": 0.0,
      "text": " with a text data type response",
      "tokens": [
        50620,
        365,
        257,
        2487,
        1412,
        2010,
        4134,
        50812
      ]
    },
    {
      "id": 180,
      "avg_logprob": -0.20296575129032135,
      "compression_ratio": 1.5650407075881958,
      "end": 571.52001953125,
      "no_speech_prob": 0.00047283980529755354,
      "seek": 55902,
      "start": 567.97998046875,
      "temperature": 0.0,
      "text": " with the text looks something like this.",
      "tokens": [
        50812,
        365,
        264,
        2487,
        1542,
        746,
        411,
        341,
        13,
        50989
      ]
    },
    {
      "id": 181,
      "avg_logprob": -0.20296575129032135,
      "compression_ratio": 1.5650407075881958,
      "end": 573.219970703125,
      "no_speech_prob": 0.00047283980529755354,
      "seek": 55902,
      "start": 571.52001953125,
      "temperature": 0.0,
      "text": " We take it and then we save it",
      "tokens": [
        50989,
        492,
        747,
        309,
        293,
        550,
        321,
        3155,
        309,
        51074
      ]
    },
    {
      "id": 182,
      "avg_logprob": -0.20296575129032135,
      "compression_ratio": 1.5650407075881958,
      "end": 576.0,
      "no_speech_prob": 0.00047283980529755354,
      "seek": 55902,
      "start": 573.219970703125,
      "temperature": 0.0,
      "text": " in a text file called audio.chapters.",
      "tokens": [
        51074,
        294,
        257,
        2487,
        3991,
        1219,
        6278,
        13,
        339,
        569,
        1559,
        13,
        51213
      ]
    },
    {
      "id": 183,
      "avg_logprob": -0.20296575129032135,
      "compression_ratio": 1.5650407075881958,
      "end": 580.0999755859375,
      "no_speech_prob": 0.00047283980529755354,
      "seek": 55902,
      "start": 578.02001953125,
      "temperature": 0.0,
      "text": " And yeah, that's pretty much all.",
      "tokens": [
        51314,
        400,
        1338,
        11,
        300,
        311,
        1238,
        709,
        439,
        13,
        51418
      ]
    },
    {
      "id": 184,
      "avg_logprob": -0.20296575129032135,
      "compression_ratio": 1.5650407075881958,
      "end": 582.6599731445312,
      "no_speech_prob": 0.00047283980529755354,
      "seek": 55902,
      "start": 580.0999755859375,
      "temperature": 0.0,
      "text": " I am going to link the code in the description below.",
      "tokens": [
        51418,
        286,
        669,
        516,
        281,
        2113,
        264,
        3089,
        294,
        264,
        3855,
        2507,
        13,
        51546
      ]
    },
    {
      "id": 185,
      "avg_logprob": -0.20296575129032135,
      "compression_ratio": 1.5650407075881958,
      "end": 585.5800170898438,
      "no_speech_prob": 0.00047283980529755354,
      "seek": 55902,
      "start": 582.6599731445312,
      "temperature": 0.0,
      "text": " So if you have any questions, just let me know.",
      "tokens": [
        51546,
        407,
        498,
        291,
        362,
        604,
        1651,
        11,
        445,
        718,
        385,
        458,
        13,
        51692
      ]
    },
    {
      "id": 186,
      "avg_logprob": -0.20296575129032135,
      "compression_ratio": 1.5650407075881958,
      "end": 587.9000244140625,
      "no_speech_prob": 0.00047283980529755354,
      "seek": 55902,
      "start": 585.5800170898438,
      "temperature": 0.0,
      "text": " Otherwise I will catch you folks in the next one.",
      "tokens": [
        51692,
        10328,
        286,
        486,
        3745,
        291,
        4024,
        294,
        264,
        958,
        472,
        13,
        51808
      ]
    },
    {
      "id": 187,
      "avg_logprob": -0.535205066204071,
      "compression_ratio": 0.7599999904632568,
      "end": 588.97998046875,
      "no_speech_prob": 0.00776281300932169,
      "seek": 58790,
      "start": 587.9000244140625,
      "temperature": 0.0,
      "text": " Take care, bye-bye.",
      "tokens": [
        50364,
        3664,
        1127,
        11,
        6543,
        12,
        6650,
        13,
        50418
      ]
    }
  ],
  "usage": {
    "seconds": 590.0,
    "type": "duration"
  },
  "words": null,
  "task": "transcribe"
}